{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b8de25",
   "metadata": {},
   "source": [
    "# Library import and Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd258b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cml.data_v1 as cmldata\n",
    "import csv, os\n",
    "\n",
    "# Sample in-code customization of spark configurations\n",
    "#from pyspark import SparkContext\n",
    "#SparkContext.setSystemProperty('spark.executor.cores', '1')\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "CONNECTION_NAME = \"pdnd-prod-dl-1\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()\n",
    "\n",
    "print(\"SparkSession created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80009d70",
   "metadata": {},
   "source": [
    "# SQL query definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76664176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "    select \n",
    "    s.id_stazione as stazione,\n",
    "    sp.creditor.idPA as EC,\n",
    "    count(*) as TRX\n",
    "    from pagopa.silver_positive sp\n",
    "    inner join pagopa.bronze_cfg_pa pa on pa.id_dominio = sp.creditor.idPA\n",
    "    inner join pagopa.bronze_cfg_pa_stazione_pa psp on psp.fk_pa = pa.obj_id\n",
    "    inner join pagopa.bronze_cfg_stazioni s on s.obj_id = psp.fk_stazione\n",
    "    where 1=1\n",
    "    and sp.paymentinfo.paymentdatetime >= CAST('2025-07-01 00:00:00' AS TIMESTAMP)\n",
    "    and sp.paymentinfo.paymentdatetime <  CAST('2025-08-01 00:00:00' AS TIMESTAMP)\n",
    "    and sp.creditor.idStation = '15376371009_51'\n",
    "    and sp.debtorposition.iuv like concat(CAST(psp.segregazione AS STRING), '%')\n",
    "    group by \n",
    "    s.id_stazione,\n",
    "    sp.creditor.idPA\n",
    "    order by TRX desc\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f0c2b",
   "metadata": {},
   "source": [
    "# Query execution and result preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark query ececution\n",
    "print(\"Spark query execution...\")\n",
    "results_df = spark.sql(sql_query)\n",
    "\n",
    "# Dataframe caching\n",
    "results_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910b9ec",
   "metadata": {},
   "source": [
    "# Generate csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"/home/cdsw/report\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "local_csv = os.path.join(local_dir, \"report_stazioni_trx.csv\")\n",
    "\n",
    "# write header from schema, then stream rows\n",
    "with open(local_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    cols = [f.name for f in results_df.schema.fields]\n",
    "    writer.writerow(cols)\n",
    "    for r in results_df.toLocalIterator():   # streams; no big collect\n",
    "        writer.writerow([r[c] for c in cols])\n",
    "\n",
    "print(f\"[OK] CSV written on driver: {local_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec4bcf",
   "metadata": {},
   "source": [
    "# Computing the 10 stations with the most transactions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calcolo delle 10 stazioni con piÃ¹ transazioni...\")\n",
    "\n",
    "# Aggregate by station, sum transactions, get top 10\n",
    "top_10_station_df = results_df.groupBy(\"stazione\") \\\n",
    "    .agg(sum(\"TRX\").alias(\"total_trx\")) \\\n",
    "    .orderBy(col(\"total_trx\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"DataFrame ready for visualizzation:\")\n",
    "top_10_station_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0971ff9",
   "metadata": {},
   "source": [
    "# Plotting libraries like Matplotlib work with local (in-memory) data, not distributed DataFrames. \n",
    "## We then need to convert our small top_10_station_df DataFrame (which only contains 10 rows) into a Pandas DataFrame using .toPandas()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3796a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convert from Spark DF into Pandas DF for plotting...\")\n",
    "top_10_pandas_df = top_10_station_df.toPandas()\n",
    "\n",
    "# Bar chart creation\n",
    "print(\"Bar chart creation...\")\n",
    "plt.style.use('ggplot') # Stile del grafico\n",
    "fig, ax = plt.subplots(figsize=(12, 8)) # Dimensioni del grafico\n",
    "\n",
    "ax.bar(top_10_pandas_df['stazione'], top_10_pandas_df['total_trx'], color='skyblue')\n",
    "\n",
    "# Add label and title\n",
    "ax.set_xlabel('ID Stazione', fontsize=12)\n",
    "ax.set_ylabel('Numero Totale di Transazioni', fontsize=12)\n",
    "ax.set_title('Top 10 Stazioni per Numero di Transazioni (Luglio 2025)', fontsize=16)\n",
    "\n",
    "# Rotate the labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show graph\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
