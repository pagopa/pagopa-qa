{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70819aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Spark session via CML data connection\n",
    "import cml.data_v1 as cmldata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "CONNECTION_NAME = \"pdnd-prod-dl-1\"   # <-- adjust if your connection is named differently\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "print(\"Spark enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67529bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Spark small to avoid hitting namespace quotas\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"32\")\n",
    "\n",
    "# If your environment allows these, they help keep executors low:\n",
    "spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "spark.conf.set(\"spark.dynamicAllocation.initialExecutors\", \"1\")\n",
    "spark.conf.set(\"spark.dynamicAllocation.maxExecutors\", \"4\")   # stay below quota\n",
    "\n",
    "# Be more patient detecting missing pods (if allowed)\n",
    "spark.conf.set(\"spark.kubernetes.executor.missingPodDetectDelta\", \"60s\")\n",
    "\n",
    "# Disable Arrow → avoids some driver/executor serialization issues in restricted clusters\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "print(\"Spark tuned for notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User-tunable knobs\n",
    "DATE_FROM      = \"2025-01-01 00:00:00\"  # lower bound on payment datetime\n",
    "SAMPLE_FRAC    = 0.02                   # 2% random sample at source (set smaller if you hit limits)\n",
    "MAX_ROWS       = 50_000                 # hard cap at source\n",
    "ML_MAX_ROWS    = 10_000                 # rows collected to pandas (keep small)\n",
    "SHOW_ROWS      = 10                     # preview rows to show\n",
    "\n",
    "print(f\"DATE_FROM={DATE_FROM}, SAMPLE_FRAC={SAMPLE_FRAC}, MAX_ROWS={MAX_ROWS}, ML_MAX_ROWS={ML_MAX_ROWS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f758bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT pa, ragione_sociale, remittance\n",
    "FROM (\n",
    "  SELECT\n",
    "    t.fiscalcodepa           AS pa,\n",
    "    t.companyname            AS ragione_sociale,\n",
    "    t.remittanceinformation  AS remittance\n",
    "  FROM pagopa.silver_positive sp\n",
    "  LATERAL VIEW EXPLODE(sp.transferlist) t_view AS t\n",
    "  WHERE t.transfercategory LIKE '%0101100IM%'\n",
    "    AND sp.paymentinfo.paymentdatetime >= CAST('{DATE_FROM}' AS TIMESTAMP)\n",
    "    AND t.remittanceinformation IS NOT NULL\n",
    "    AND t.remittanceinformation <> ''\n",
    ") base\n",
    "WHERE rand() <= {SAMPLE_FRAC}\n",
    "LIMIT {MAX_ROWS}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running Spark SQL with sampling+limit pushdown…\")\n",
    "df = spark.sql(query).select(\n",
    "    F.col(\"pa\").cast(\"string\").alias(\"pa\"),\n",
    "    F.col(\"ragione_sociale\").cast(\"string\").alias(\"ragione_sociale\"),\n",
    "    F.col(\"remittance\").cast(\"string\").alias(\"remittance\")\n",
    ")\n",
    "\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print(\"Preview:\")\n",
    "df.show(SHOW_ROWS, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7581a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: keep ML sample small to avoid driver OOM and quota pressure\n",
    "df_small = df.limit(ML_MAX_ROWS)\n",
    "\n",
    "print(\"Collecting a small slice to pandas…\")\n",
    "pdf = df_small.toPandas()\n",
    "print(f\"Pandas rows: {len(pdf)}\")\n",
    "pdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Try NLTK stopwords if available; fall back to a small built-in list otherwise\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    it_stop = set(stopwords.words('italian'))\n",
    "    print(\"Using NLTK Italian stopwords.\")\n",
    "except Exception:\n",
    "    it_stop = {\n",
    "        \"a\",\"ad\",\"al\",\"alla\",\"alle\",\"allo\",\"ai\",\"agli\",\"agli\",\"all\",\"con\",\"col\",\"coi\",\"come\",\n",
    "        \"da\",\"dal\",\"dalla\",\"dalle\",\"dallo\",\"dei\",\"del\",\"della\",\"delle\",\"dello\",\n",
    "        \"di\",\"e\",\"ed\",\"gli\",\"i\",\"il\",\"in\",\"la\",\"le\",\"lo\",\"l\",\"ma\",\"mi\",\"ne\",\"nei\",\"nella\",\n",
    "        \"nelle\",\"nello\",\"no\",\"non\",\"o\",\"ogni\",\"per\",\"piu\",\"poi\",\"qua\",\"quale\",\"quali\",\"quello\",\n",
    "        \"quella\",\"quelle\",\"quelli\",\"se\",\"sia\",\"siano\",\"si\",\"sono\",\"sul\",\"sulla\",\"sulle\",\"sullo\",\n",
    "        \"su\",\"tra\",\"un\",\"una\",\"uno\",\"voi\",\"noi\",\"voi\",\"è\",\"e'\",\"d\",\"l\",\"s\",\"c\",\"t\"\n",
    "    }\n",
    "    print(\"Using fallback Italian stopwords (compact list).\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    # keep letters+spaces, replace others with space\n",
    "    s = re.sub(r\"[^a-zàèéìòóùç\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def preprocess(s: str) -> str:\n",
    "    s = normalize_text(s)\n",
    "    tokens = [tok for tok in s.split() if tok not in it_stop and len(tok) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"Preprocessing sample:\")\n",
    "pdf[\"remit_clean\"] = pdf[\"remittance\"].astype(str).apply(preprocess)\n",
    "pdf[[\"remittance\", \"remit_clean\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d701ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# TF-IDF with modest bounds to keep matrix small\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=5)  # tweak if your sample is very small/large\n",
    "X = vectorizer.fit_transform(pdf[\"remit_clean\"])\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "\n",
    "# Choose a small k; you can iterate later\n",
    "K = 8\n",
    "print(f\"Training KMeans with k={K}…\")\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=\"auto\")\n",
    "kmeans.fit(X)\n",
    "\n",
    "pdf[\"cluster\"] = kmeans.labels_\n",
    "print(\"Cluster distribution:\")\n",
    "print(pdf[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "order_centroids = np.argsort(kmeans.cluster_centers_, axis=1)[:, ::-1]\n",
    "\n",
    "TOP_N = 10\n",
    "SAMPLES_PER_CLUSTER = 3\n",
    "\n",
    "for i in range(K):\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Cluster {i} | size={ (pdf['cluster']==i).sum() }\")\n",
    "    top_words = [terms[idx] for idx in order_centroids[i, :TOP_N]]\n",
    "    print(\"Top words:\", \", \".join(top_words))\n",
    "\n",
    "    examples = pdf.loc[pdf[\"cluster\"]==i, \"remittance\"].head(SAMPLES_PER_CLUSTER).tolist()\n",
    "    for e in examples:\n",
    "        print(\"  •\", e[:200].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster assignments and top words in project workspace (local)\n",
    "assign_path = \"remittance_clusters_sample.csv\"\n",
    "pdf[[\"pa\",\"ragione_sociale\",\"remittance\",\"cluster\"]].to_csv(assign_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote: {assign_path}\")\n",
    "\n",
    "# Top words table\n",
    "rows = []\n",
    "for i in range(K):\n",
    "    top_words = [terms[idx] for idx in order_centroids[i, :10]]\n",
    "    rows.append({\"cluster\": i, \"top_words\": \", \".join(top_words)})\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(rows).to_csv(\"cluster_top_words.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Wrote: cluster_top_words.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
